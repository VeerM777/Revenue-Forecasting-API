{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12300813,"sourceType":"datasetVersion","datasetId":7753163}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install fastapi uvicorn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T12:02:14.105453Z","iopub.execute_input":"2025-06-27T12:02:14.105764Z","iopub.status.idle":"2025-06-27T12:02:20.097567Z","shell.execute_reply.started":"2025-06-27T12:02:14.105741Z","shell.execute_reply":"2025-06-27T12:02:20.096610Z"}},"outputs":[{"name":"stdout","text":"Collecting fastapi\n  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\nCollecting uvicorn\n  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\nRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\nRequirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\nRequirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\nDownloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\nSuccessfully installed fastapi-0.115.14 starlette-0.46.2 uvicorn-0.34.3\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom datetime import datetime\nimport uvicorn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T12:02:22.394119Z","iopub.execute_input":"2025-06-27T12:02:22.394430Z","iopub.status.idle":"2025-06-27T12:02:23.255340Z","shell.execute_reply.started":"2025-06-27T12:02:22.394402Z","shell.execute_reply":"2025-06-27T12:02:23.254260Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class PredictionInput(BaseModel):\n    date: str\n    product_category: str\n    region: str\n    agent_id: str\n    marketing_spend: float\n    lead_count: int","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T12:02:25.979181Z","iopub.execute_input":"2025-06-27T12:02:25.979647Z","iopub.status.idle":"2025-06-27T12:02:25.986081Z","shell.execute_reply.started":"2025-06-27T12:02:25.979623Z","shell.execute_reply":"2025-06-27T12:02:25.984784Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Analyzing the data\nimport pandas as pd\n\ndef check_data_issues(df):\n    \"\"\"Checks dataset for issues like missing values, invalid dates, datatypes, and duplicates.\"\"\"\n    print(\"Data Issues Report:\")\n    print(f\"Total Rows: {len(df)}\")\n    \n    # 1. Missing Values\n    missing = df.isna().sum()\n    print(\"\\nMissing Values (isna()):\")\n    for col, count in missing.items():\n        if count > 0:\n            print(f\"- {col}: {count} missing values\")\n    \n    # 2. Invalid Categorical Values ('Null', 'N/A', empty)\n    categorical_cols = ['product_category', 'region', 'agent_id']\n    print(\"\\nInvalid Categorical Values ('Null', 'N/A', empty):\")\n    for col in categorical_cols:\n        if col in df.columns:\n            invalid = df[col].isin(['Null', 'N/A', '']).sum()\n            if invalid > 0:\n                print(f\"- {col}: {invalid} invalid values\")\n        else:\n            print(f\"- {col}: Column not found\")\n    \n    # 3. Invalid Dates\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'], format='%d-%m-%Y', errors='coerce')\n        invalid_dates = df['date'].isna().sum() + (df['date'].dt.year != 2025).sum()\n        print(f\"\\nInvalid Dates (unparseable or not in 2025): {invalid_dates}\")\n    else:\n        print(\"\\nInvalid Dates: 'date' column not found\")\n    \n    # 4. Non-Numeric Values in Numeric Columns\n    numeric_cols = ['marketing_spend', 'lead_count', 'revenue']\n    print(\"\\nNon-Numeric Values in Numeric Columns:\")\n    for col in numeric_cols:\n        if col in df.columns:\n            non_numeric = pd.to_numeric(df[col], errors='coerce').isna().sum()\n            if non_numeric > 0:\n                print(f\"- {col}: {non_numeric} non-numeric values\")\n            negative = (pd.to_numeric(df[col], errors='coerce') < 0).sum()\n            if negative > 0:\n                print(f\"- {col}: {negative} negative values\")\n        else:\n            print(f\"- {col}: Column not found\")\n    \n    # 5. Duplicates\n    duplicates = df.duplicated().sum()\n    print(f\"\\nDuplicate Rows (duplicated()): {duplicates}\")\n\nif __name__ == \"__main__\":\n    # Load CSV file directly\n    df = pd.read_csv(\"/kaggle/input/forecasting-dataset/dummy_revenue_forecasting_data.csv\")\n    check_data_issues(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T12:02:31.173200Z","iopub.execute_input":"2025-06-27T12:02:31.173521Z","iopub.status.idle":"2025-06-27T12:02:34.223350Z","shell.execute_reply.started":"2025-06-27T12:02:31.173497Z","shell.execute_reply":"2025-06-27T12:02:34.222365Z"}},"outputs":[{"name":"stdout","text":"Data Issues Report:\nTotal Rows: 610998\n\nMissing Values (isna()):\n- agent_id: 22242 missing values\n- marketing_spend: 1080 missing values\n- lead_count: 1080 missing values\n- revenue: 1080 missing values\n\nInvalid Categorical Values ('Null', 'N/A', empty):\n- region: 6602 invalid values\n- agent_id: 2 invalid values\n\nInvalid Dates (unparseable or not in 2025): 156890\n\nNon-Numeric Values in Numeric Columns:\n- marketing_spend: 12810 non-numeric values\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in less\n  return op(a, b)\n","output_type":"stream"},{"name":"stdout","text":"- lead_count: 1080 non-numeric values\n- revenue: 6562 non-numeric values\n\nDuplicate Rows (duplicated()): 620\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df[features]\ny = df['revenue']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:37:13.856911Z","iopub.execute_input":"2025-06-27T13:37:13.857271Z","iopub.status.idle":"2025-06-27T13:37:14.794950Z","shell.execute_reply.started":"2025-06-27T13:37:13.857247Z","shell.execute_reply":"2025-06-27T13:37:14.793794Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from xgboost import XGBRegressor\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T13:59:50.396589Z","iopub.execute_input":"2025-06-27T13:59:50.396984Z","iopub.status.idle":"2025-06-27T13:59:50.705671Z","shell.execute_reply.started":"2025-06-27T13:59:50.396957Z","shell.execute_reply":"2025-06-27T13:59:50.704401Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# ✅ Weekly Revenue Forecasting Pipeline\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_absolute_percentage_error\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nimport joblib\n\n# Load dataset\nfile_path = \"/kaggle/input/forecasting-dataset/dummy_revenue_forecasting_data.csv\"\ndf = pd.read_csv(file_path)\n\n# Parse and filter date\ndf['date'] = pd.to_datetime(df['date'], errors='coerce', dayfirst=True)\ndf = df[df['date'].dt.year == 2025]\n\n# Clean & convert columns\ndf.replace({'region': {'Null': np.nan, 'N/A': np.nan, '': np.nan},\n            'agent_id': {'Null': np.nan, 'N/A': np.nan, '': np.nan}}, inplace=True)\nfor col in ['marketing_spend', 'lead_count', 'revenue']:\n    df[col] = pd.to_numeric(df[col], errors='coerce')\n\ndf.dropna(subset=['date', 'product_category', 'region', 'agent_id',\n                  'marketing_spend', 'lead_count', 'revenue'], inplace=True)\ndf.drop_duplicates(inplace=True)\n\n# 🔄 Weekly aggregation\ndf['week'] = df['date'].dt.to_period('W').apply(lambda r: r.start_time)\nweekly_df = df.groupby(['week', 'agent_id', 'product_category', 'region']).agg({\n    'marketing_spend': 'sum',\n    'lead_count': 'sum',\n    'revenue': 'sum'\n}).reset_index()\n\n# Sort\nweekly_df.sort_values(by=['agent_id', 'week'], inplace=True)\n\n# Lag features\nfor lag in [1, 2, 3]:\n    weekly_df[f'revenue_lag_{lag}'] = weekly_df.groupby('agent_id')['revenue'].shift(lag)\n    weekly_df[f'marketing_lag_{lag}'] = weekly_df.groupby('agent_id')['marketing_spend'].shift(lag)\n    weekly_df[f'leads_lag_{lag}'] = weekly_df.groupby('agent_id')['lead_count'].shift(lag)\n\n# Rolling means\nweekly_df['revenue_roll_mean_3'] = weekly_df.groupby('agent_id')['revenue'].transform(lambda x: x.shift(1).rolling(3).mean())\n\n# Temporal features\nweekly_df['weekofyear'] = weekly_df['week'].dt.isocalendar().week\nweekly_df['month'] = weekly_df['week'].dt.month\n\n# Encoding\nfor col in ['product_category', 'region', 'agent_id']:\n    weekly_df[f'{col}_freq'] = weekly_df[col].map(weekly_df[col].value_counts(normalize=True))\n    weekly_df[f'{col}_mean_rev'] = weekly_df[col].map(weekly_df.groupby(col)['revenue'].mean())\n\n# Drop NA from lag features\nweekly_df.dropna(inplace=True)\n\n# Log target\ny = np.log1p(weekly_df['revenue'])\nfeatures = [\n    'marketing_spend', 'lead_count',\n    'revenue_lag_1', 'revenue_lag_2', 'revenue_lag_3', 'revenue_roll_mean_3',\n    'marketing_lag_1', 'marketing_lag_2', 'marketing_lag_3',\n    'leads_lag_1', 'leads_lag_2', 'leads_lag_3',\n    'weekofyear', 'month',\n    'product_category_freq', 'region_freq', 'agent_id_freq',\n    'product_category_mean_rev', 'region_mean_rev', 'agent_id_mean_rev'\n]\nX = weekly_df[features]\n\n# Split\ntsplit = int(len(X) * 0.8)\nX_train, X_test = X.iloc[:tsplit], X.iloc[tsplit:]\ny_train, y_test = y.iloc[:tsplit], y.iloc[tsplit:]\n\n# Models\ncat = CatBoostRegressor(verbose=0, iterations=2000, learning_rate=0.03, depth=6, random_seed=42)\nlgb = LGBMRegressor(n_estimators=2000, learning_rate=0.03, num_leaves=50, random_state=42)\nxgb = XGBRegressor(n_estimators=2000, learning_rate=0.03, max_depth=6, tree_method='hist', subsample=0.8,\n                   colsample_bytree=0.8, random_state=42)\n\n# Stacking\nstack = StackingRegressor(\n    estimators=[('cat', cat), ('lgb', lgb), ('xgb', xgb)],\n    final_estimator=LGBMRegressor(n_estimators=500, learning_rate=0.05, num_leaves=30, random_state=42),\n    passthrough=True\n)\n\n# Fit\nstack.fit(X_train, y_train)\n\n# Predict\ny_pred_log = stack.predict(X_test)\ny_pred = np.expm1(y_pred_log)\ny_true = np.expm1(y_test)\n\n# Metrics\nprint(f\"✅ MAE: {mean_absolute_error(y_true, y_pred):.2f}\")\nprint(f\"✅ R²: {r2_score(y_true, y_pred):.4f}\")\nprint(f\"✅ MAPE: {mean_absolute_percentage_error(y_true, y_pred):.4f}\")\n\n# Save\njoblib.dump(stack, \"/kaggle/working/final_model2.pkl\")\njoblib.dump(features, \"/kaggle/working/final_features2.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:44:56.190722Z","iopub.execute_input":"2025-06-27T16:44:56.191050Z","iopub.status.idle":"2025-06-27T16:47:01.740469Z","shell.execute_reply.started":"2025-06-27T16:44:56.191027Z","shell.execute_reply":"2025-06-27T16:47:01.739659Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000539 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3100\n[LightGBM] [Info] Number of data points in the train set: 2216, number of used features: 17\n[LightGBM] [Info] Start training from score 16.294248\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000459 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3099\n[LightGBM] [Info] Number of data points in the train set: 1772, number of used features: 17\n[LightGBM] [Info] Start training from score 16.295174\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000226 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3099\n[LightGBM] [Info] Number of data points in the train set: 1773, number of used features: 17\n[LightGBM] [Info] Start training from score 16.300653\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000234 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3100\n[LightGBM] [Info] Number of data points in the train set: 1773, number of used features: 17\n[LightGBM] [Info] Start training from score 16.283293\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3099\n[LightGBM] [Info] Number of data points in the train set: 1773, number of used features: 17\n[LightGBM] [Info] Start training from score 16.299834\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000243 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3099\n[LightGBM] [Info] Number of data points in the train set: 1773, number of used features: 17\n[LightGBM] [Info] Start training from score 16.292285\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000322 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3865\n[LightGBM] [Info] Number of data points in the train set: 2216, number of used features: 20\n[LightGBM] [Info] Start training from score 16.294248\n✅ MAE: 343603.62\n✅ R²: 0.9771\n✅ MAPE: 0.0290\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/final_features2.pkl']"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import joblib\nimport numpy as np\nimport pandas as pd\n\n# Load the model and features\nmodel = joblib.load(\"/kaggle/working/final_model2.pkl\")\nfeatures = joblib.load(\"/kaggle/working/final_features2.pkl\")\n\n# ✅ Example input (you can modify values here)\nexample_input = {\n   \"marketing_spend\": 45000,\n    \"lead_count\": 350,\n    \"revenue_lag_1\": 420000,\n    \"revenue_lag_2\": 390000,\n    \"revenue_lag_3\": 370000,\n    \"revenue_roll_mean_3\": 393333.33,\n    \"marketing_lag_1\": 40000,\n    \"marketing_lag_2\": 38000,\n    \"marketing_lag_3\": 36000,\n    \"leads_lag_1\": 300,\n    \"leads_lag_2\": 280,\n    \"leads_lag_3\": 260,\n    \"weekofyear\": 27,\n    \"month\": 7,\n    \"product_category_freq\": 0.12,\n    \"region_freq\": 0.18,\n    \"agent_id_freq\": 0.004,\n    \"product_category_mean_rev\": 410000,\n    \"region_mean_rev\": 395000,\n    \"agent_id_mean_rev\": 385000\n}\n\n# Convert to DataFrame\nX_new = pd.DataFrame([example_input])[features]\n\n# Predict (model was trained on log1p)\nlog_pred = model.predict(X_new)\nrevenue_pred = np.expm1(log_pred)[0]\n\nprint(f\"✅ Forecasted Weekly Revenue: ₹{revenue_pred:,.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T17:08:12.021265Z","iopub.execute_input":"2025-06-27T17:08:12.021641Z","iopub.status.idle":"2025-06-27T17:08:12.204121Z","shell.execute_reply.started":"2025-06-27T17:08:12.021614Z","shell.execute_reply":"2025-06-27T17:08:12.203062Z"}},"outputs":[{"name":"stdout","text":"✅ Forecasted Weekly Revenue: ₹4,092,376.67\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"df.head(20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T17:07:10.871609Z","iopub.execute_input":"2025-06-27T17:07:10.874475Z","iopub.status.idle":"2025-06-27T17:07:10.926784Z","shell.execute_reply.started":"2025-06-27T17:07:10.874360Z","shell.execute_reply":"2025-06-27T17:07:10.925799Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         date product_category region agent_id  marketing_spend  lead_count  \\\n0  2025-03-29         Cosmetic  North       A4          21000.0       170.0   \n1  2025-03-29         Logistic  South       A5          14000.0        60.0   \n2  2025-03-29          Fintech   West       A5          31000.0        70.0   \n3  2025-03-30         Cosmetic  North       A3          47000.0        70.0   \n4  2025-03-30             FMCG   West       A9          19000.0        80.0   \n5  2025-03-30             FMCG   East       A8           5000.0       120.0   \n6  2025-03-31             FMCG   East       A3           5000.0       170.0   \n7  2025-03-31        Insurance  South       A3          20000.0       140.0   \n8  2025-03-31          Fintech  South       A9          34000.0       190.0   \n9  2025-04-01        Insurance   East       A5          22000.0       180.0   \n10 2025-04-01          Fintech  South      A10           3000.0        20.0   \n11 2025-04-01         Logistic  South       A3          13000.0       160.0   \n12 2025-04-02          Fintech   East       A8          42000.0        40.0   \n13 2025-04-02        Insurance  North       A3          18000.0        20.0   \n14 2025-04-02          Fintech   West       A4          13000.0        30.0   \n15 2025-04-03         Logistic   West       A9          12000.0        20.0   \n16 2025-04-03         Cosmetic   East       A5           7000.0        20.0   \n17 2025-04-03        Insurance   East       A4          24000.0       200.0   \n18 2025-04-04             FMCG  North       A8          11000.0       100.0   \n19 2025-04-04        Insurance  North       A9          37000.0       150.0   \n\n    revenue       week  \n0   75000.0 2025-03-24  \n1   43000.0 2025-03-24  \n2   89000.0 2025-03-24  \n3   86000.0 2025-03-24  \n4   74000.0 2025-03-24  \n5   86000.0 2025-03-24  \n6   70000.0 2025-03-31  \n7   72000.0 2025-03-31  \n8   51000.0 2025-03-31  \n9   71000.0 2025-03-31  \n10  49000.0 2025-03-31  \n11  43000.0 2025-03-31  \n12  71000.0 2025-03-31  \n13  57000.0 2025-03-31  \n14  92000.0 2025-03-31  \n15  53000.0 2025-03-31  \n16  75000.0 2025-03-31  \n17  60000.0 2025-03-31  \n18  49000.0 2025-03-31  \n19  72000.0 2025-03-31  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>product_category</th>\n      <th>region</th>\n      <th>agent_id</th>\n      <th>marketing_spend</th>\n      <th>lead_count</th>\n      <th>revenue</th>\n      <th>week</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-03-29</td>\n      <td>Cosmetic</td>\n      <td>North</td>\n      <td>A4</td>\n      <td>21000.0</td>\n      <td>170.0</td>\n      <td>75000.0</td>\n      <td>2025-03-24</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-03-29</td>\n      <td>Logistic</td>\n      <td>South</td>\n      <td>A5</td>\n      <td>14000.0</td>\n      <td>60.0</td>\n      <td>43000.0</td>\n      <td>2025-03-24</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-03-29</td>\n      <td>Fintech</td>\n      <td>West</td>\n      <td>A5</td>\n      <td>31000.0</td>\n      <td>70.0</td>\n      <td>89000.0</td>\n      <td>2025-03-24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2025-03-30</td>\n      <td>Cosmetic</td>\n      <td>North</td>\n      <td>A3</td>\n      <td>47000.0</td>\n      <td>70.0</td>\n      <td>86000.0</td>\n      <td>2025-03-24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2025-03-30</td>\n      <td>FMCG</td>\n      <td>West</td>\n      <td>A9</td>\n      <td>19000.0</td>\n      <td>80.0</td>\n      <td>74000.0</td>\n      <td>2025-03-24</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2025-03-30</td>\n      <td>FMCG</td>\n      <td>East</td>\n      <td>A8</td>\n      <td>5000.0</td>\n      <td>120.0</td>\n      <td>86000.0</td>\n      <td>2025-03-24</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2025-03-31</td>\n      <td>FMCG</td>\n      <td>East</td>\n      <td>A3</td>\n      <td>5000.0</td>\n      <td>170.0</td>\n      <td>70000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2025-03-31</td>\n      <td>Insurance</td>\n      <td>South</td>\n      <td>A3</td>\n      <td>20000.0</td>\n      <td>140.0</td>\n      <td>72000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2025-03-31</td>\n      <td>Fintech</td>\n      <td>South</td>\n      <td>A9</td>\n      <td>34000.0</td>\n      <td>190.0</td>\n      <td>51000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2025-04-01</td>\n      <td>Insurance</td>\n      <td>East</td>\n      <td>A5</td>\n      <td>22000.0</td>\n      <td>180.0</td>\n      <td>71000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2025-04-01</td>\n      <td>Fintech</td>\n      <td>South</td>\n      <td>A10</td>\n      <td>3000.0</td>\n      <td>20.0</td>\n      <td>49000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>2025-04-01</td>\n      <td>Logistic</td>\n      <td>South</td>\n      <td>A3</td>\n      <td>13000.0</td>\n      <td>160.0</td>\n      <td>43000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>2025-04-02</td>\n      <td>Fintech</td>\n      <td>East</td>\n      <td>A8</td>\n      <td>42000.0</td>\n      <td>40.0</td>\n      <td>71000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>2025-04-02</td>\n      <td>Insurance</td>\n      <td>North</td>\n      <td>A3</td>\n      <td>18000.0</td>\n      <td>20.0</td>\n      <td>57000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>2025-04-02</td>\n      <td>Fintech</td>\n      <td>West</td>\n      <td>A4</td>\n      <td>13000.0</td>\n      <td>30.0</td>\n      <td>92000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>2025-04-03</td>\n      <td>Logistic</td>\n      <td>West</td>\n      <td>A9</td>\n      <td>12000.0</td>\n      <td>20.0</td>\n      <td>53000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>2025-04-03</td>\n      <td>Cosmetic</td>\n      <td>East</td>\n      <td>A5</td>\n      <td>7000.0</td>\n      <td>20.0</td>\n      <td>75000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>2025-04-03</td>\n      <td>Insurance</td>\n      <td>East</td>\n      <td>A4</td>\n      <td>24000.0</td>\n      <td>200.0</td>\n      <td>60000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>2025-04-04</td>\n      <td>FMCG</td>\n      <td>North</td>\n      <td>A8</td>\n      <td>11000.0</td>\n      <td>100.0</td>\n      <td>49000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2025-04-04</td>\n      <td>Insurance</td>\n      <td>North</td>\n      <td>A9</td>\n      <td>37000.0</td>\n      <td>150.0</td>\n      <td>72000.0</td>\n      <td>2025-03-31</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12}]}